{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e349f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Setting up LSTM Time Embedding Comparison...\n",
      "DEBUG: Triton Kernels Available: True\n",
      "ðŸ”§ Using device: cuda\n",
      "âœ… Setup complete! All imports and model definitions loaded.\n",
      "âœ… Device: cuda\n",
      "âœ… Results directory: /home/peera/kan-mammote/results\n",
      "ðŸš€ Ready to run the comparison!\n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ LSTM Time Embedding Comparison - Complete Setup Cell\n",
    "print(\"ðŸ”§ Setting up LSTM Time Embedding Comparison...\")\n",
    "\n",
    "# Core imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep learning imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Data processing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.auto import tqdm # Import tqdm\n",
    "\n",
    "# Add project root to path\n",
    "project_root = '/home/peera/kan-mammote'\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import project modules\n",
    "from src.models.kan_mammote import KANMAMMOTE\n",
    "from src.utils.config import KANMAMMOTEConfig\n",
    "from src.LETE.LeTE import CombinedLeTE as LeTE # Import CombinedLeTE and alias it as LeTE\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "LSTM_HIDDEN_DIM = 128\n",
    "TIME_EMBEDDING_DIM = 64\n",
    "DROPOUT_RATE = 0.2\n",
    "THRESHOLD = 0.3\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "\n",
    "# Setup device and directories\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸ”§ Using device: {device}\")\n",
    "\n",
    "RESULTS_DIR = f\"{project_root}/results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "class EventBasedMNIST(Dataset):\n",
    "    \"\"\"Convert MNIST to event-based representation.\"\"\"\n",
    "    \n",
    "    def __init__(self, root, train=True, threshold=0.3, download=False):\n",
    "        self.threshold = threshold\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self.mnist = datasets.MNIST(root=root, train=train, download=download, transform=transform)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.mnist)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.mnist[idx]\n",
    "        \n",
    "        # Convert to event representation\n",
    "        image = image.squeeze().numpy()\n",
    "        \n",
    "        # Create events for pixels above threshold\n",
    "        events = []\n",
    "        features = []\n",
    "        \n",
    "        for i in range(28):\n",
    "            for j in range(28):\n",
    "                if image[i, j] > self.threshold:\n",
    "                    position = i * 28 + j  # Flatten position\n",
    "                    events.append(position)\n",
    "                    features.append(image[i, j])\n",
    "        \n",
    "        if len(events) == 0:\n",
    "            events = [0]\n",
    "            features = [0.0]\n",
    "        \n",
    "        return np.array(events), np.array(features), label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for variable length sequences.\"\"\"\n",
    "    events_batch = []\n",
    "    features_batch = []\n",
    "    lengths = []\n",
    "    labels = []\n",
    "    \n",
    "    max_len = max(len(events) for events, _, _ in batch)\n",
    "    \n",
    "    for events, features, label in batch:\n",
    "        length = len(events)\n",
    "        lengths.append(length)\n",
    "        labels.append(label)\n",
    "        \n",
    "        # Pad sequences\n",
    "        padded_events = np.zeros(max_len, dtype=np.int64)\n",
    "        padded_features = np.zeros(max_len, dtype=np.float32)\n",
    "        \n",
    "        padded_events[:length] = events\n",
    "        padded_features[:length] = features\n",
    "        \n",
    "        events_batch.append(padded_events)\n",
    "        features_batch.append(padded_features)\n",
    "    \n",
    "    return (torch.tensor(events_batch), \n",
    "            torch.tensor(features_batch), \n",
    "            torch.tensor(lengths), \n",
    "            torch.tensor(labels))\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "class TrueBaselineLSTM(nn.Module):\n",
    "    \"\"\"True baseline LSTM without any time embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden_dim=128, num_classes=10, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Just use feature values directly\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=1,  # Just the pixel values\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, events, features, lengths):\n",
    "        if features.size(0) == 0:\n",
    "            return torch.zeros(0, 10, device=features.device)\n",
    "        \n",
    "        lengths = torch.clamp(lengths, min=1, max=features.size(1))\n",
    "        \n",
    "        # Use only pixel values, no positional information\n",
    "        x = features.unsqueeze(-1)  # Add feature dimension\n",
    "        \n",
    "        # Pack sequences\n",
    "        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, c_n) = self.lstm(packed)\n",
    "        \n",
    "        # Classify using last hidden state\n",
    "        output = self.classifier(h_n[-1])\n",
    "        return output\n",
    "\n",
    "class LearnablePositionLSTM(nn.Module):\n",
    "    \"\"\"LSTM with learnable position embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden_dim=128, num_classes=10, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Learnable position embedding\n",
    "        self.position_embedding = nn.Embedding(input_size, TIME_EMBEDDING_DIM)\n",
    "        \n",
    "        # Feature projection\n",
    "        self.feature_proj = nn.Linear(1, TIME_EMBEDDING_DIM)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=TIME_EMBEDDING_DIM,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, events, features, lengths):\n",
    "        if events.size(0) == 0:\n",
    "            return torch.zeros(0, 10, device=events.device)\n",
    "        \n",
    "        lengths = torch.clamp(lengths, min=1, max=events.size(1))\n",
    "        \n",
    "        # Get position embeddings\n",
    "        pos_emb = self.position_embedding(events)\n",
    "        \n",
    "        # Project features\n",
    "        feat_emb = self.feature_proj(features.unsqueeze(-1))\n",
    "        \n",
    "        # Combine position and feature embeddings\n",
    "        combined = pos_emb + feat_emb\n",
    "        \n",
    "        # Pack sequences\n",
    "        packed = pack_padded_sequence(combined, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, c_n) = self.lstm(packed)\n",
    "        \n",
    "        # Classify\n",
    "        output = self.classifier(h_n[-1])\n",
    "        return output\n",
    "\n",
    "class SinCosLSTM(nn.Module):\n",
    "    \"\"\"LSTM with sinusoidal position embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden_dim=128, num_classes=10, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Feature projection\n",
    "        self.feature_proj = nn.Linear(1, TIME_EMBEDDING_DIM)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=TIME_EMBEDDING_DIM,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def get_sincos_embeddings(self, positions):\n",
    "        \"\"\"Generate sinusoidal position embeddings.\"\"\"\n",
    "        batch_size, seq_len = positions.shape\n",
    "        embeddings = torch.zeros(batch_size, seq_len, TIME_EMBEDDING_DIM, device=positions.device)\n",
    "        \n",
    "        # Normalize positions to [0, 1]\n",
    "        max_pos = positions.max(dim=1, keepdim=True)[0].float()\n",
    "        normalized_pos = positions.float() / (max_pos + 1e-8)\n",
    "        \n",
    "        div_term = torch.exp(torch.arange(0, TIME_EMBEDDING_DIM, 2, device=positions.device).float() * \n",
    "                            -(np.log(10000.0) / TIME_EMBEDDING_DIM))\n",
    "        \n",
    "        pos_scaled = normalized_pos.unsqueeze(-1) * div_term.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        embeddings[:, :, 0::2] = torch.sin(pos_scaled)\n",
    "        embeddings[:, :, 1::2] = torch.cos(pos_scaled)\n",
    "        \n",
    "        return embeddings\n",
    "        \n",
    "    def forward(self, events, features, lengths):\n",
    "        if events.size(0) == 0:\n",
    "            return torch.zeros(0, 10, device=events.device)\n",
    "        \n",
    "        lengths = torch.clamp(lengths, min=1, max=events.size(1))\n",
    "        \n",
    "        # Get sinusoidal position embeddings\n",
    "        pos_emb = self.get_sincos_embeddings(events)\n",
    "        \n",
    "        # Project features\n",
    "        feat_emb = self.feature_proj(features.unsqueeze(-1))\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined = pos_emb + feat_emb\n",
    "        \n",
    "        # Pack sequences\n",
    "        packed = pack_padded_sequence(combined, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, c_n) = self.lstm(packed)\n",
    "        \n",
    "        # Classify\n",
    "        output = self.classifier(h_n[-1])\n",
    "        return output\n",
    "\n",
    "class LETE_LSTM_Fixed(nn.Module):\n",
    "    \"\"\"LSTM with LETE time embeddings - FIXED.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden_dim=128, num_classes=10, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes # Ensure this line is present (fixed in last response)\n",
    "        \n",
    "        # Initialize LETE\n",
    "        self.lete = LeTE(dim=TIME_EMBEDDING_DIM)\n",
    "        \n",
    "        # Feature projection\n",
    "        self.feature_proj = nn.Linear(1, TIME_EMBEDDING_DIM)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=TIME_EMBEDDING_DIM,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, events, features, lengths):\n",
    "        # Input validation\n",
    "        if events.size(0) == 0:\n",
    "            return torch.zeros(0, self.num_classes, device=events.device)\n",
    "        \n",
    "        # Clamp lengths to prevent issues\n",
    "        lengths = torch.clamp(lengths, min=1, max=events.size(1))\n",
    "        \n",
    "        # Process valid sequences only\n",
    "        valid_mask = lengths > 0\n",
    "        if not valid_mask.any():\n",
    "            return torch.zeros(events.size(0), self.num_classes, device=events.device)\n",
    "        \n",
    "        events_valid = events[valid_mask]\n",
    "        features_valid = features[valid_mask]\n",
    "        lengths_valid = lengths[valid_mask]\n",
    "\n",
    "        # Normalize timestamps (result shape: [valid_batch_size, seq_len])\n",
    "        max_pos = events_valid.max(dim=1, keepdim=True)[0].float()\n",
    "        max_pos[max_pos == 0] = 1.0 # Prevent division by zero for empty sequences\n",
    "        timestamps_norm = events_valid.float() / (max_pos + 1e-8) # Added small epsilon for numerical stability\n",
    "        \n",
    "        # DEBUG CHECK 1: Check timestamps_norm\n",
    "        if torch.isnan(timestamps_norm).any() or torch.isinf(timestamps_norm).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in timestamps_norm. Range: [{timestamps_norm.min().item():.4f}, {timestamps_norm.max().item():.4f}]\")\n",
    "\n",
    "        # Get LETE embeddings\n",
    "        lete_emb = self.lete(timestamps_norm) \n",
    "        \n",
    "        # DEBUG CHECK 2: lete_emb (output of CombinedLeTE) - Most likely place for NaNs\n",
    "        if torch.isnan(lete_emb).any() or torch.isinf(lete_emb).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in lete_emb (output of CombinedLeTE). Range: [{lete_emb.min().item():.4f}, {lete_emb.max().item():.4f}]\")\n",
    "        \n",
    "        # Ensure shape matches, if not, something is fundamentally wrong in CombinedLeTE\n",
    "        batch_size_valid, seq_len_valid = timestamps_norm.shape\n",
    "        if lete_emb.shape != (batch_size_valid, seq_len_valid, TIME_EMBEDDING_DIM):\n",
    "            raise ValueError(f\"CombinedLeTE output shape mismatch: Expected \"\n",
    "                             f\"({batch_size_valid}, {seq_len_valid}, {TIME_EMBEDDING_DIM}), \"\n",
    "                             f\"got {lete_emb.shape}\")\n",
    "\n",
    "        # Clip to prevent numerical issues (good practice, can prevent exploding values from propagating)\n",
    "        lete_emb = torch.clamp(lete_emb, min=-1e5, max=1e5) # Increased range for a bit more robustness\n",
    "        \n",
    "        # Project features\n",
    "        feat_emb = self.feature_proj(features_valid.unsqueeze(-1))\n",
    "\n",
    "        # DEBUG CHECK 3: feat_emb\n",
    "        if torch.isnan(feat_emb).any() or torch.isinf(feat_emb).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in feat_emb. Range: [{feat_emb.min().item():.4f}, {feat_emb.max().item():.4f}]\")\n",
    "        \n",
    "        # Combine embeddings\n",
    "        combined_valid = lete_emb + feat_emb\n",
    "        \n",
    "        # DEBUG CHECK 4: combined_valid after addition\n",
    "        if torch.isnan(combined_valid).any() or torch.isinf(combined_valid).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in combined_valid after addition. Range: [{combined_valid.min().item():.4f}, {combined_valid.max().item():.4f}]\")\n",
    "            \n",
    "        combined_valid = F.relu(combined_valid) # ReLU is generally safe but can pass Inf\n",
    "        \n",
    "        # DEBUG CHECK 5: combined_valid after ReLU\n",
    "        if torch.isnan(combined_valid).any() or torch.isinf(combined_valid).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in combined_valid after ReLU. Range: [{combined_valid.min().item():.4f}, {combined_valid.max().item():.4f}]\")\n",
    "        \n",
    "        # Apply sequence mask\n",
    "        seq_mask = torch.arange(combined_valid.size(1), device=lengths_valid.device)[None, :] < lengths_valid[:, None]\n",
    "        combined_valid = combined_valid * seq_mask.unsqueeze(-1).float()\n",
    "        \n",
    "        # DEBUG CHECK 6: combined_valid after masking\n",
    "        if torch.isnan(combined_valid).any() or torch.isinf(combined_valid).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in combined_valid after masking. Range: [{combined_valid.min().item():.4f}, {combined_valid.max().item():.4f}]\")\n",
    "        \n",
    "        # LSTM processing\n",
    "        packed = pack_padded_sequence(combined_valid, lengths_valid.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, c_n) = self.lstm(packed) # LSTM can propagate NaNs if inputs are extreme\n",
    "        \n",
    "        # DEBUG CHECK 7: h_n (LSTM hidden state)\n",
    "        if torch.isnan(h_n).any() or torch.isinf(h_n).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in h_n (LSTM hidden state). Range: [{h_n.min().item():.4f}, {h_n.max().item():.4f}]\")\n",
    "        \n",
    "        # Classifier\n",
    "        valid_logits = self.classifier(h_n[-1])\n",
    "        \n",
    "        # DEBUG CHECK 8: valid_logits (classifier output)\n",
    "        if torch.isnan(valid_logits).any() or torch.isinf(valid_logits).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in valid_logits (classifier output). Range: [{valid_logits.min().item():.4f}, {valid_logits.max().item():.4f}]\")\n",
    "        \n",
    "        # Create full output tensor and place valid logits back\n",
    "        full_logits = torch.zeros(events.size(0), self.num_classes, device=events.device)\n",
    "        full_logits[valid_mask] = valid_logits\n",
    "        \n",
    "        return full_logits\n",
    "class KAN_MAMMOTE_LSTM_Fixed(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_dim=128, num_classes=10, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes # ADDED: Ensure num_classes is stored as an attribute\n",
    "        \n",
    "        kan_mammote_internal_feature_dim = 16 \n",
    "        self.kan_mammote_input_feature_proj = nn.Linear(1, kan_mammote_internal_feature_dim)\n",
    "        \n",
    "        self.kan_config = KANMAMMOTEConfig(\n",
    "            d_model=TIME_EMBEDDING_DIM,\n",
    "            D_time=TIME_EMBEDDING_DIM, \n",
    "            input_feature_dim=kan_mammote_internal_feature_dim,\n",
    "            output_dim_for_task=TIME_EMBEDDING_DIM,\n",
    "            K_top=4,\n",
    "            use_aux_features_router=False,\n",
    "            raw_event_feature_dim=0,\n",
    "            num_layers=1,\n",
    "            lambda_sobolev_l2=0.0,\n",
    "            lambda_total_variation=0.0,\n",
    "        )\n",
    "        \n",
    "        self.kan_mammote = KANMAMMOTE(self.kan_config)\n",
    "        \n",
    "        self.feature_proj = nn.Linear(1, TIME_EMBEDDING_DIM)\n",
    "        \n",
    "        self.combine = nn.Linear(TIME_EMBEDDING_DIM * 2, TIME_EMBEDDING_DIM)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=TIME_EMBEDDING_DIM, \n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, events, features, lengths):\n",
    "        # Initial validation and masking (unchanged, good for handling empty batches)\n",
    "        if events.size(0) == 0:\n",
    "            return torch.zeros(0, self.num_classes, device=events.device)\n",
    "        \n",
    "        lengths = torch.clamp(lengths, min=1, max=events.size(1))\n",
    "        \n",
    "        valid_mask = lengths > 0\n",
    "        if not valid_mask.any():\n",
    "            return torch.zeros(events.size(0), self.num_classes, device=events.device)\n",
    "        \n",
    "        events_valid = events[valid_mask]\n",
    "        features_valid = features[valid_mask]\n",
    "        lengths_valid = lengths[valid_mask]\n",
    "        \n",
    "        # Normalize timestamps\n",
    "        max_pos = events_valid.max(dim=1, keepdim=True)[0].float()\n",
    "        max_pos[max_pos == 0] = 1.0 # Avoid division by zero for empty sequences\n",
    "        timestamps_norm = events_valid.float() / (max_pos + 1e-8) # Added epsilon for safety\n",
    "        \n",
    "        # DEBUG CHECK: Check timestamps_norm\n",
    "        if torch.isnan(timestamps_norm).any() or torch.isinf(timestamps_norm).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in timestamps_norm. Range: [{timestamps_norm.min().item():.4f}, {timestamps_norm.max().item():.4f}]\")\n",
    "\n",
    "        # Project input features for KAN-MAMMOTE\n",
    "        kan_mammote_input_features = self.kan_mammote_input_feature_proj(features_valid.unsqueeze(-1))\n",
    "        \n",
    "        # DEBUG CHECK: Check kan_mammote_input_features\n",
    "        if torch.isnan(kan_mammote_input_features).any() or torch.isinf(kan_mammote_input_features).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in kan_mammote_input_features. Range: [{kan_mammote_input_features.min().item():.4f}, {kan_mammote_input_features.max().item():.4f}]\")\n",
    "\n",
    "        # Process through KAN-MAMMOTE\n",
    "        kan_embeddings, kan_mammote_losses = self.kan_mammote(\n",
    "            timestamps=timestamps_norm,\n",
    "            features=kan_mammote_input_features,\n",
    "            auxiliary_features=None\n",
    "        )\n",
    "        \n",
    "        # DEBUG CHECK: kan_embeddings (This is a very common place for NaNs to appear in custom complex models)\n",
    "        if torch.isnan(kan_embeddings).any() or torch.isinf(kan_embeddings).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in kan_embeddings (output of KANMAMMOTE). Range: [{kan_embeddings.min().item():.4f}, {kan_embeddings.max().item():.4f}]\")\n",
    "        \n",
    "        # Project original pixel features for the parallel path\n",
    "        feat_emb = self.feature_proj(features_valid.unsqueeze(-1))\n",
    "        \n",
    "        # DEBUG CHECK: feat_emb\n",
    "        if torch.isnan(feat_emb).any() or torch.isinf(feat_emb).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in feat_emb. Range: [{feat_emb.min().item():.4f}, {feat_emb.max().item():.4f}]\")\n",
    "        \n",
    "        # Combine embeddings from both pathways\n",
    "        combined_valid = torch.cat([kan_embeddings, feat_emb], dim=-1)\n",
    "        \n",
    "        # DEBUG CHECK: combined_valid after concat\n",
    "        if torch.isnan(combined_valid).any() or torch.isinf(combined_valid).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in combined_valid after concat. Range: [{combined_valid.min().item():.4f}, {combined_valid.max().item():.4f}]\")\n",
    "\n",
    "        combined_valid = self.combine(combined_valid)\n",
    "        \n",
    "        # DEBUG CHECK: combined_valid after self.combine\n",
    "        if torch.isnan(combined_valid).any() or torch.isinf(combined_valid).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in combined_valid after self.combine. Range: [{combined_valid.min().item():.4f}, {combined_valid.max().item():.4f}]\")\n",
    "            \n",
    "        combined_valid = F.relu(combined_valid) # ReLU is generally safe, but can propagate Inf\n",
    "        \n",
    "        # DEBUG CHECK: combined_valid after ReLU\n",
    "        if torch.isnan(combined_valid).any() or torch.isinf(combined_valid).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in combined_valid after ReLU. Range: [{combined_valid.min().item():.4f}, {combined_valid.max().item():.4f}]\")\n",
    "        \n",
    "        # Apply sequence mask\n",
    "        batch_size_valid, max_seq_len_valid = combined_valid.shape[0], combined_valid.shape[1]\n",
    "        seq_mask = torch.arange(max_seq_len_valid, device=lengths_valid.device)[None, :] < lengths_valid[:, None]\n",
    "        combined_valid = combined_valid * seq_mask.unsqueeze(-1).float()\n",
    "        \n",
    "        # DEBUG CHECK: combined_valid after masking\n",
    "        if torch.isnan(combined_valid).any() or torch.isinf(combined_valid).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in combined_valid after masking. Range: [{combined_valid.min().item():.4f}, {combined_valid.max().item():.4f}]\")\n",
    "        \n",
    "        # LSTM processing\n",
    "        packed = pack_padded_sequence(combined_valid, lengths_valid.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, c_n) = self.lstm(packed) # LSTM can propagate NaNs if inputs are extreme\n",
    "        \n",
    "        # DEBUG CHECK: h_n (LSTM hidden state)\n",
    "        if torch.isnan(h_n).any() or torch.isinf(h_n).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in h_n (LSTM hidden state). Range: [{h_n.min().item():.4f}, {h_n.max().item():.4f}]\")\n",
    "        \n",
    "        # Classifier\n",
    "        valid_logits = self.classifier(h_n[-1])\n",
    "        \n",
    "        # DEBUG CHECK: valid_logits (classifier output)\n",
    "        if torch.isnan(valid_logits).any() or torch.isinf(valid_logits).any():\n",
    "            raise ValueError(f\"NaN/Inf detected in valid_logits (classifier output). Range: [{valid_logits.min().item():.4f}, {valid_logits.max().item():.4f}]\")\n",
    "        \n",
    "        # Create full output tensor and place valid logits back\n",
    "        full_logits = torch.zeros(events.size(0), self.num_classes, device=events.device)\n",
    "        full_logits[valid_mask] = valid_logits\n",
    "        \n",
    "        return full_logits\n",
    "        \n",
    "def train_model(model, train_loader, test_loader, model_name):\n",
    "    \"\"\"Train a single model with immediate error stopping.\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [],\n",
    "        'epochs': [], 'training_time': []\n",
    "    }\n",
    "    \n",
    "    best_test_acc = 0.0\n",
    "    \n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\", leave=False)\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (events, features, lengths, labels) in enumerate(train_pbar):\n",
    "            # No try-except here - let errors propagate and stop execution\n",
    "            events, features, lengths, labels = events.to(device), features.to(device), lengths.to(device), labels.to(device)\n",
    "            \n",
    "            # Debug info for first few batches\n",
    "            '''if batch_idx < 3:\n",
    "                print(f\"\\nðŸ“Š Batch {batch_idx} debug:\")\n",
    "                print(f\"  - events: shape={events.shape}, range=[{events.min().item()}, {events.max().item()}]\")\n",
    "                print(f\"  - features: shape={features.shape}, range=[{features.min().item()}, {features.max().item()}]\")\n",
    "                print(f\"  - lengths: shape={lengths.shape}, range=[{lengths.min().item()}, {lengths.max().item()}]\")\n",
    "                print(f\"  - labels: shape={labels.shape}, values={labels.tolist()}\")'''\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(events, features, lengths)\n",
    "            \n",
    "            # Check outputs and raise error if issues are found\n",
    "            if torch.isnan(outputs).any():\n",
    "                print(f\"âš ï¸ NaN detected in outputs at batch {batch_idx}!\")\n",
    "                raise ValueError(f\"NaN detected in model outputs at batch {batch_idx}\")\n",
    "                \n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Check loss and raise error if issues are found\n",
    "            if torch.isnan(loss) or torch.isinf(loss):\n",
    "                print(f\"âš ï¸ NaN/Inf loss detected at batch {batch_idx}: {loss.item()}\")\n",
    "                raise ValueError(f\"NaN or Inf loss value detected at batch {batch_idx}: {loss.item()}\")\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Check gradients and raise error if issues are found\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None and (torch.isnan(param.grad).any() or torch.isinf(param.grad).any()):\n",
    "                    print(f\"âš ï¸ NaN/Inf gradient detected in {name}\")\n",
    "                    raise ValueError(f\"NaN or Inf gradient detected in parameter {name}\")\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Update tqdm postfix\n",
    "            train_pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{100*train_correct/train_total:.2f}%\")\n",
    "        \n",
    "        train_pbar.close()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        test_pbar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Test]\", leave=False)\n",
    "        test_loss = 0.0\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (events, features, lengths, labels) in enumerate(test_pbar):\n",
    "                # No try-except here - let errors propagate and stop execution\n",
    "                events, features, lengths, labels = events.to(device), features.to(device), lengths.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(events, features, lengths)\n",
    "                \n",
    "                # Check outputs and raise error if issues are found\n",
    "                if torch.isnan(outputs).any():\n",
    "                    print(f\"âš ï¸ NaN detected in test outputs at batch {batch_idx}!\")\n",
    "                    raise ValueError(f\"NaN detected in test model outputs at batch {batch_idx}\")\n",
    "                    \n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Check loss and raise error if issues are found\n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"âš ï¸ NaN/Inf test loss detected at batch {batch_idx}: {loss.item()}\")\n",
    "                    raise ValueError(f\"NaN or Inf test loss value detected at batch {batch_idx}: {loss.item()}\")\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                test_total += labels.size(0)\n",
    "                test_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                test_pbar.set_postfix(loss=f\"{loss.item():.4f}\", acc=f\"{100*test_correct/test_total:.2f}%\")\n",
    "        \n",
    "        test_pbar.close()\n",
    "\n",
    "        # Calculate metrics\n",
    "        epoch_time = time.time() - start_time\n",
    "        train_acc = 100 * train_correct / max(train_total, 1)  # Avoid division by zero\n",
    "        test_acc = 100 * test_correct / max(test_total, 1)    # Avoid division by zero\n",
    "        avg_train_loss = train_loss / max(len(train_loader), 1)  # Avoid division by zero\n",
    "        avg_test_loss = test_loss / max(len(test_loader), 1)    # Avoid division by zero\n",
    "        \n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_loss'].append(avg_test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['epochs'].append(epoch + 1)\n",
    "        history['training_time'].append(epoch_time)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{NUM_EPOCHS}: Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%, Time: {epoch_time:.1f}s')\n",
    "    \n",
    "    print(f\"âœ… {model_name} training complete. Best test accuracy: {best_test_acc:.2f}%\")\n",
    "    return history, best_test_acc\n",
    "\n",
    "def plot_training_curves(results):\n",
    "    \"\"\"Plot training curves for all models.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot training accuracy\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for model_name, history in results.items():\n",
    "        if 'train_acc' in history and len(history['train_acc']) > 0:\n",
    "            plt.plot(history['epochs'], history['train_acc'], label=model_name, marker='o')\n",
    "    plt.title('Training Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot test accuracy\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for model_name, history in results.items():\n",
    "        if 'test_acc' in history and len(history['test_acc']) > 0:\n",
    "            plt.plot(history['epochs'], history['test_acc'], label=model_name, marker='o')\n",
    "    plt.title('Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for model_name, history in results.items():\n",
    "        if 'train_loss' in history and len(history['train_loss']) > 0:\n",
    "            plt.plot(history['epochs'], history['train_loss'], label=model_name, marker='o')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot test loss\n",
    "    plt.subplot(2, 2, 4)\n",
    "    for model_name, history in results.items():\n",
    "        if 'test_loss' in history and len(history['test_loss']) > 0:\n",
    "            plt.plot(history['epochs'], history['test_loss'], label=model_name, marker='o')\n",
    "    plt.title('Test Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{RESULTS_DIR}/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"âœ… Training curves saved to {RESULTS_DIR}/training_curves.png\")\n",
    "\n",
    "print(\"âœ… Setup complete! All imports and model definitions loaded.\")\n",
    "print(f\"âœ… Device: {device}\")\n",
    "print(f\"âœ… Results directory: {RESULTS_DIR}\")\n",
    "print(\"ðŸš€ Ready to run the comparison!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de4abe",
   "metadata": {},
   "source": [
    "Looking at the error trace, I can see there's a shape mismatch error occurring during the matrix multiplication in the neural network. The error \"mat1 and mat2 shapes cannot be multiplied (32x16 and 1x64)\" suggests that your KAN-MAMMOTE model has a dimension mismatch between the projected timestamps and the network's expected input shape.\n",
    "\n",
    "Let me create a test file to help diagnose this issue. I'll focus on testing the model initialization, forward pass, and the key component causing the error.\n",
    "\n",
    "\n",
    "\n",
    "Made changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23b87960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Running on GPU.\n"
     ]
    }
   ],
   "source": [
    "#check cuda\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Running on GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87564c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence # Make sure pad_packed_sequence is imported if needed, but pack_padded_sequence is the main one for input.\n",
    "import numpy as np # For omega calculation\n",
    "\n",
    "# Assuming global constants like BATCH_SIZE, LEARNING_RATE, NUM_EPOCHS,\n",
    "# LSTM_HIDDEN_DIM, TIME_EMBEDDING_DIM, DROPOUT_RATE, THRESHOLD, GRAD_CLIP_NORM\n",
    "# are already defined in test_compare.ipynb\n",
    "\n",
    "class TGATLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    LSTM with time embedding based on the formula:\n",
    "    sqrt(1/d_T) * [cos(omega_1(t-t') + phi_1), ..., cos(omega_d(t-t') + phi_d)]\n",
    "    where d_T is TIME_EMBEDDING_DIM, phi are learnable, and t-t' is the time difference.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=784, hidden_dim=128, num_classes=10, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes # Store num_classes\n",
    "        self.time_embedding_dim = TIME_EMBEDDING_DIM # d_T from the formula\n",
    "        \n",
    "        # Learnable phase shifts (phi_i)\n",
    "        # Initialized uniformly to avoid bias\n",
    "        self.phi = nn.Parameter(torch.empty(self.time_embedding_dim))\n",
    "        nn.init.uniform_(self.phi, a=-np.pi, b=np.pi) # Phases typically range from -pi to pi\n",
    "\n",
    "        # Fixed frequencies (omega_i)\n",
    "        # Using an exponential progression, common in Fourier features\n",
    "        # Example: 1 / (10000^(2i/d_model)) or similar for varying frequencies\n",
    "        # Here we adapt to 1/(1000^(i/d_T)) for relative time.\n",
    "        self.omega = 1.0 / (1000**(torch.arange(0, self.time_embedding_dim, dtype=torch.float32) / self.time_embedding_dim))\n",
    "        # self.omega will be a 1D tensor of shape (TIME_EMBEDDING_DIM,)\n",
    "        self.register_buffer('omega_buffer', self.omega) # Store as buffer, not learnable\n",
    "\n",
    "        # Feature projection: Projects pixel intensity from 1D to TIME_EMBEDDING_DIM\n",
    "        self.feature_proj = nn.Linear(1, self.time_embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.time_embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, events, features, lengths):\n",
    "        # Handle empty batches and clamp lengths (standard from test_compare.ipynb)\n",
    "        if events.size(0) == 0:\n",
    "            return torch.zeros(0, self.num_classes, device=events.device)\n",
    "        \n",
    "        lengths = torch.clamp(lengths, min=1, max=events.size(1))\n",
    "        \n",
    "        valid_mask = lengths > 0\n",
    "        if not valid_mask.any():\n",
    "            return torch.zeros(events.size(0), self.num_classes, device=events.device)\n",
    "        \n",
    "        events_valid = events[valid_mask]\n",
    "        features_valid = features[valid_mask]\n",
    "        lengths_valid = lengths[valid_mask]\n",
    "\n",
    "        # 1. Calculate Time Differences (t - t')\n",
    "        # We need delta_t for each element. For the first element, t' is implicitly 0.\n",
    "        # So, delta_t[0] = events[0], delta_t[i] = events[i] - events[i-1] for i > 0.\n",
    "        \n",
    "        # Create a tensor for differences, initially same as events_valid\n",
    "        delta_t = events_valid.clone().float() # Ensure float type\n",
    "        \n",
    "        # Calculate differences for elements from the second one onwards\n",
    "        # Using roll to shift events_valid by one position to get previous values\n",
    "        # The first element will remain events_valid[:, 0]\n",
    "        # For padded parts, this will also compute diffs of zeros, which will then be zeroed out by masking.\n",
    "        delta_t[:, 1:] = events_valid[:, 1:].float() - events_valid[:, :-1].float()\n",
    "        \n",
    "        # DEBUG CHECK: delta_t (for time differences)\n",
    "        # if torch.isnan(delta_t).any() or torch.isinf(delta_t).any():\n",
    "        #     raise ValueError(f\"NaN/Inf detected in delta_t. Range: [{delta_t.min().item():.4f}, {delta_t.max().item():.4f}]\")\n",
    "        \n",
    "        # 2. Generate Fourier Time Embedding\n",
    "        # Apply omega and phi to delta_t\n",
    "        # delta_t_expanded: (batch_size, seq_len, 1) -> to allow broadcasting with omega (1D) and phi (1D)\n",
    "        delta_t_expanded = delta_t.unsqueeze(-1) # Shape: (B, S, 1)\n",
    "        \n",
    "        # Element-wise multiplication with omega_buffer (B, S, D_T)\n",
    "        scaled_delta_t = delta_t_expanded * self.omega_buffer\n",
    "        \n",
    "        # Add learnable phase shifts (B, S, D_T)\n",
    "        arg_to_cos = scaled_delta_t + self.phi\n",
    "        \n",
    "        # Compute cosine (B, S, D_T)\n",
    "        time_emb = torch.cos(arg_to_cos)\n",
    "        \n",
    "        # Apply scaling factor sqrt(1/d_T)\n",
    "        scaling_factor = 1.0 / math.sqrt(self.time_embedding_dim)\n",
    "        time_emb = time_emb * scaling_factor\n",
    "\n",
    "        # DEBUG CHECK: time_emb (output of Fourier embedding)\n",
    "        # if torch.isnan(time_emb).any() or torch.isinf(time_emb).any():\n",
    "        #     raise ValueError(f\"NaN/Inf detected in time_emb. Range: [{time_emb.min().item():.4f}, {time_emb.max().item():.4f}]\")\n",
    "\n",
    "        # 3. Project features\n",
    "        feat_emb = self.feature_proj(features_valid.unsqueeze(-1))\n",
    "        \n",
    "        # DEBUG CHECK: feat_emb\n",
    "        # if torch.isnan(feat_emb).any() or torch.isinf(feat_emb).any():\n",
    "        #     raise ValueError(f\"NaN/Inf detected in feat_emb. Range: [{feat_emb.min().item():.4f}, {feat_emb.max().item():.4f}]\")\n",
    "        \n",
    "        # 4. Combine embeddings\n",
    "        combined_valid = time_emb + feat_emb\n",
    "        combined_valid = F.relu(combined_valid) # Apply activation after combination\n",
    "        \n",
    "        # DEBUG CHECK: combined_valid after combination\n",
    "        # if torch.isnan(combined_valid).any() or torch.isinf(combined_valid).any():\n",
    "        #     raise ValueError(f\"NaN/Inf detected in combined_valid after combination. Range: [{combined_valid.min().item():.4f}, {combined_valid.max().item():.4f}]\")\n",
    "        \n",
    "        # Apply sequence mask to zero out padded elements in combined embeddings\n",
    "        batch_size_valid, max_seq_len_valid = combined_valid.shape[0], combined_valid.shape[1]\n",
    "        seq_mask = torch.arange(max_seq_len_valid, device=lengths_valid.device)[None, :] < lengths_valid[:, None]\n",
    "        combined_valid = combined_valid * seq_mask.unsqueeze(-1).float()\n",
    "        \n",
    "        # 5. LSTM processing\n",
    "        packed = pack_padded_sequence(combined_valid, lengths_valid.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (h_n, c_n) = self.lstm(packed)\n",
    "        \n",
    "        # DEBUG CHECK: h_n (LSTM hidden state)\n",
    "        # if torch.isnan(h_n).any() or torch.isinf(h_n).any():\n",
    "        #     raise ValueError(f\"NaN/Inf detected in h_n (LSTM hidden state). Range: [{h_n.min().item():.4f}, {h_n.max().item():.4f}]\")\n",
    "        \n",
    "        # 6. Classify\n",
    "        valid_logits = self.classifier(h_n[-1])\n",
    "        \n",
    "        # DEBUG CHECK: valid_logits (classifier output)\n",
    "        # if torch.isnan(valid_logits).any() or torch.isinf(valid_logits).any():\n",
    "        #     raise ValueError(f\"NaN/Inf detected in valid_logits (classifier output). Range: [{valid_logits.min().item():.4f}, {valid_logits.max().item():.4f}]\")\n",
    "        \n",
    "        # Create full output tensor and place valid logits back\n",
    "        full_logits = torch.zeros(events.size(0), self.num_classes, device=events.device)\n",
    "        full_logits[valid_mask] = valid_logits\n",
    "        \n",
    "        return full_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94bd0b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "298d066b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting LSTM Time Embedding Comparison - FIXED VERSION (STOP ON ERROR)...\n",
      "\n",
      "ðŸ“ Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:02<00:00, 3.90MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 174kB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:01<00:00, 1.10MB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 4.16MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded: 60000 train, 10000 test samples\n",
      "Initializing KAN-MAMMOTE with config: <src.utils.config.KANMAMMOTEConfig object at 0x729fcc5e8450>\n",
      "\n",
      "ðŸ“Š Model Information:\n",
      "   True_Baseline: 200,458 parameters\n",
      "   TimeDiff_Fourier_LSTM: 232,906 parameters\n",
      "   SinCos_LSTM: 232,842 parameters\n",
      "   LETE_LSTM: 252,650 parameters\n",
      "   KAN_MAMMOTE_LSTM: 309,918 parameters\n",
      "\n",
      "==================================================\n",
      "Starting training for True_Baseline\n",
      "==================================================\n",
      "Training True_Baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: Train Acc: 25.96%, Test Acc: 41.70%, Time: 77.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: Train Acc: 54.06%, Test Acc: 65.86%, Time: 67.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 174\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;66;03m# Run the comparison\u001b[39;00m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 94\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m     93\u001b[39m \u001b[38;5;66;03m# No try-except here - let errors propagate and stop execution\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m history, best_acc = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m results[model_name] = history\n\u001b[32m     96\u001b[39m best_accuracies[model_name] = best_acc\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 556\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, test_loader, model_name)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''if batch_idx < 3:\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[33;03m    print(f\"\\nðŸ“Š Batch {batch_idx} debug:\")\u001b[39;00m\n\u001b[32m    549\u001b[39m \u001b[33;03m    print(f\"  - events: shape={events.shape}, range=[{events.min().item()}, {events.max().item()}]\")\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[33;03m    print(f\"  - features: shape={features.shape}, range=[{features.min().item()}, {features.max().item()}]\")\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[33;03m    print(f\"  - lengths: shape={lengths.shape}, range=[{lengths.min().item()}, {lengths.max().item()}]\")\u001b[39;00m\n\u001b[32m    552\u001b[39m \u001b[33;03m    print(f\"  - labels: shape={labels.shape}, values={labels.tolist()}\")'''\u001b[39;00m\n\u001b[32m    554\u001b[39m optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[38;5;66;03m# Check outputs and raise error if issues are found\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.isnan(outputs).any():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kanmote_wsl/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kanmote_wsl/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 151\u001b[39m, in \u001b[36mTrueBaselineLSTM.forward\u001b[39m\u001b[34m(self, events, features, lengths)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;66;03m# Pack sequences\u001b[39;00m\n\u001b[32m    150\u001b[39m packed = pack_padded_sequence(x, lengths.cpu(), batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_sorted=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m _, (h_n, c_n) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# Classify using last hidden state\u001b[39;00m\n\u001b[32m    154\u001b[39m output = \u001b[38;5;28mself\u001b[39m.classifier(h_n[-\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kanmote_wsl/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kanmote_wsl/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/kanmote_wsl/lib/python3.11/site-packages/torch/nn/modules/rnn.py:1136\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1124\u001b[39m     result = _VF.lstm(\n\u001b[32m   1125\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1126\u001b[39m         hx,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1133\u001b[39m         \u001b[38;5;28mself\u001b[39m.batch_first,\n\u001b[32m   1134\u001b[39m     )\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1136\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1141\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1142\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1147\u001b[39m output = result[\u001b[32m0\u001b[39m]\n\u001b[32m   1148\u001b[39m hidden = result[\u001b[32m1\u001b[39m:]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function with improved error handling - STOP ON ERROR version.\"\"\"\n",
    "    print(\"ðŸš€ Starting LSTM Time Embedding Comparison - FIXED VERSION (STOP ON ERROR)...\")\n",
    "    \n",
    "    try:\n",
    "        # Create datasets\n",
    "        print(\"\\nðŸ“ Loading datasets...\")\n",
    "        train_dataset = EventBasedMNIST(root='./data', train=True, threshold=THRESHOLD, download=True)\n",
    "        test_dataset = EventBasedMNIST(root='./data', train=False, threshold=THRESHOLD, download=True)\n",
    "        \n",
    "        # FIXED: Validate datasets\n",
    "        if len(train_dataset) == 0 or len(test_dataset) == 0:\n",
    "            raise ValueError(\"Empty dataset!\")\n",
    "        \n",
    "        print(f\"âœ… Dataset loaded: {len(train_dataset)} train, {len(test_dataset)} test samples\")\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERROR LOADING DATASET - STOPPING: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return  # Stop execution\n",
    "    \n",
    "    # Define models to compare\n",
    "    \"\"\"\n",
    "    'Learnable_Position': LearnablePositionLSTM(\n",
    "        input_size=784,\n",
    "        hidden_dim=LSTM_HIDDEN_DIM,\n",
    "        num_classes=10,\n",
    "        dropout=DROPOUT_RATE\n",
    "    ),\"\"\"\n",
    "        \n",
    "    models = {\n",
    "\n",
    "        'True_Baseline': TrueBaselineLSTM(\n",
    "            input_size=784,\n",
    "            hidden_dim=LSTM_HIDDEN_DIM,\n",
    "            num_classes=10,\n",
    "            dropout=DROPOUT_RATE\n",
    "        ),\n",
    "        'TimeDiff_Fourier_LSTM': TGATLSTM( # <--- ADD THIS LINE\n",
    "            input_size=784,\n",
    "            hidden_dim=LSTM_HIDDEN_DIM,\n",
    "            num_classes=10,\n",
    "            dropout=DROPOUT_RATE\n",
    "        ),\n",
    "        \n",
    "        'SinCos_LSTM': SinCosLSTM(\n",
    "            input_size=784,\n",
    "            hidden_dim=LSTM_HIDDEN_DIM,\n",
    "            num_classes=10,\n",
    "            dropout=DROPOUT_RATE\n",
    "        ),\n",
    "        \n",
    "        'LETE_LSTM': LETE_LSTM_Fixed(\n",
    "            input_size=784,\n",
    "            hidden_dim=LSTM_HIDDEN_DIM,\n",
    "            num_classes=10,\n",
    "            dropout=DROPOUT_RATE\n",
    "        ),\n",
    "        \n",
    "        'KAN_MAMMOTE_LSTM': KAN_MAMMOTE_LSTM_Fixed(\n",
    "            input_size=784,\n",
    "            hidden_dim=LSTM_HIDDEN_DIM,\n",
    "            num_classes=10,\n",
    "            dropout=DROPOUT_RATE\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Print model information\n",
    "    print(\"\\nðŸ“Š Model Information:\")\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            param_count = count_parameters(model)\n",
    "            print(f\"   {name}: {param_count:,} parameters\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ERROR COUNTING PARAMETERS FOR {name} - STOPPING: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return  # Stop execution if model parameter counting fails\n",
    "    \n",
    "    # Train all models\n",
    "    results = {}\n",
    "    best_accuracies = {}\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\n\" + \"=\"*50)\n",
    "        print(f\"Starting training for {model_name}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # No try-except here - let errors propagate and stop execution\n",
    "        history, best_acc = train_model(model, train_loader, test_loader, model_name)\n",
    "        results[model_name] = history\n",
    "        best_accuracies[model_name] = best_acc\n",
    "        \n",
    "        # Clean up memory after each model\n",
    "        del model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save results\n",
    "    print(\"\\nðŸ’¾ Saving results...\")\n",
    "    \n",
    "    try:\n",
    "        # Save training histories\n",
    "        with open(f\"{RESULTS_DIR}/training_histories.json\", 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        # Save summary results\n",
    "        summary = {\n",
    "            'best_accuracies': best_accuracies,\n",
    "            'model_parameters': {name: count_parameters(models[name]) if name in models else 0 for name in best_accuracies.keys()},\n",
    "            'configuration': {\n",
    "                'batch_size': BATCH_SIZE,\n",
    "                'learning_rate': LEARNING_RATE,\n",
    "                'num_epochs': NUM_EPOCHS,\n",
    "                'lstm_hidden_dim': LSTM_HIDDEN_DIM,\n",
    "                'time_embedding_dim': TIME_EMBEDDING_DIM,\n",
    "                'dropout_rate': DROPOUT_RATE,\n",
    "                'threshold': THRESHOLD\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(f\"{RESULTS_DIR}/summary.json\", 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        # Create CSV summary\n",
    "        with open(f\"{RESULTS_DIR}/results_summary.csv\", 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['Model', 'Best_Accuracy', 'Parameters', 'Avg_Time_per_Epoch'])\n",
    "            \n",
    "            for model_name in best_accuracies.keys():\n",
    "                if model_name in results and len(results[model_name]['training_time']) > 0:\n",
    "                    avg_time = np.mean(results[model_name]['training_time'])\n",
    "                    params = count_parameters(models[model_name])  # Fixed parameter name\n",
    "                    writer.writerow([\n",
    "                        model_name,\n",
    "                        f\"{best_accuracies[model_name]:.4f}\",\n",
    "                        params,\n",
    "                        f\"{avg_time:.2f}\"\n",
    "                    ])\n",
    "        \n",
    "        # Create visualizations\n",
    "        if results:\n",
    "            plot_training_curves(results)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ERROR SAVING RESULTS - STOPPING: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return  # Stop execution\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\nðŸŽ¯ FINAL RESULTS SUMMARY:\")\n",
    "    print(\"=\" * 80)\n",
    "    for model_name, acc in best_accuracies.items():\n",
    "        params = count_parameters(models[model_name])\n",
    "        if model_name in results and len(results[model_name]['training_time']) > 0:\n",
    "            avg_time = np.mean(results[model_name]['training_time'])\n",
    "            print(f\"{model_name:20s}: {acc:.4f} acc | {params:7,} params | {avg_time:.1f}s/epoch\")\n",
    "    \n",
    "    # Find best model\n",
    "    if best_accuracies and any(acc > 0 for acc in best_accuracies.values()):\n",
    "        best_model = max(best_accuracies, key=best_accuracies.get)\n",
    "        print(f\"\\nðŸ† Best Model: {best_model} (Accuracy: {best_accuracies[best_model]:.4f})\")\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ All results saved to: {RESULTS_DIR}\")\n",
    "    print(\"ðŸŽ‰ Comparison complete!\")\n",
    "\n",
    "# Run the comparison\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a39cb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235ab43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "#check cuda\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Running on GPU.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c0dff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kanmote_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
